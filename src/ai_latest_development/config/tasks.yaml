performance_task:
  description: >
    Use atlas_performance_tool to fetch ALL clusters and 6-hour performance measurements.
    For EACH cluster and process, analyze: CPU usage patterns, disk IOPS trends, memory utilization,
    network I/O (bytes in/out), connection counts, and operations per second (query, insert, update, delete).

    CRITICALLY IMPORTANT - INDEX ANALYSIS:
    The tool now provides THREE sources of index-related insights:
    1. **Performance Advisor Suggested Indexes** - MongoDB's built-in recommendations for missing indexes
    2. **Slow Query Logs** - queries taking >100ms that may need optimization
    3. **Index Efficiency Analysis** - scan ratio metrics (scanned objects vs documents returned)
    
    For EACH process, you MUST analyze:
    - Suggested indexes from Performance Advisor (namespace, index keys, average query execution time savings)
    - Slow queries (namespace, query shape, execution time, documents examined vs returned)
    - Index efficiency ratios (>10 = poor/missing indexes, 3-10 = fair, <3 = good)
    - Collections performing full collection scans
    - Query patterns that would benefit from compound indexes

    Create a DETAILED Markdown report with:
    1. Executive Summary - overall performance health across all clusters
    2. Per-Cluster Analysis - dedicated section for each cluster with:
       - Current configuration (instance size, region, replica count)
       - Resource utilization metrics with actual numbers and percentages
       - Performance bottlenecks identified (high CPU >75%, disk saturation >80%, slow queries)
       - Comparison of provisioned vs actual usage
    3. **INDEX OPTIMIZATION SECTION** (CRITICAL) - For EACH cluster/process:
       - List ALL suggested indexes with specific field names and expected impact
       - Identify slow queries with execution times and query shapes
       - Report scan efficiency ratios and problematic collections
       - Prioritize index recommendations by impact (query time savings)
       - Provide exact CREATE INDEX commands ready to execute
    4. Critical Issues - list of urgent performance problems requiring immediate attention
    5. Optimization Recommendations - 7-10 specific, actionable recommendations with expected impact
       (MUST include index creation recommendations with specific field names)

    Be thorough and data-driven. Include actual metric values, not just generic statements.
    If Performance Advisor data is unavailable (M0/M2 clusters), clearly state this limitation.
  expected_output: >
    Comprehensive Markdown performance report (minimum 400 words) with executive summary,
    detailed per-cluster analysis with metrics, INDEX OPTIMIZATION SECTION with specific
    CREATE INDEX commands, critical issues list, and 7-10 specific optimization recommendations.
    Must include actual numbers from metrics and specific index recommendations with field names.
  agent: performance_agent

security_task:
  description: >
    Perform a COMPREHENSIVE security audit using TWO tools:
    
    1. **atlas_security_tool** - Audit MongoDB Atlas infrastructure security
    2. **mongodb_compliance_tool** - Scan actual database collections for data compliance violations
    
    YOU MUST USE BOTH TOOLS to provide complete security coverage.
    
    **INFRASTRUCTURE SECURITY (atlas_security_tool):**
    1. IP Access List - examine every entry, flag 0.0.0.0/0 or overly broad ranges, assess risk level
    2. Database Users & Roles - list ALL users, their roles, databases they access, identify excessive privileges
    3. TLS/SSL Configuration - verify TLS version (minimum TLS 1.2), check connection string enforcement
    4. Encryption at Rest - determine if enabled, check for external KMS (AWS/Azure/GCP), flag if using default keys
    5. Network Security - check VPC peering, private endpoints, public exposure
    6. Authentication Methods - review SCRAM, X.509, LDAP configurations

    **DATA COMPLIANCE SCAN (mongodb_compliance_tool):**
    Scan ALL collections for:
    
    1. **PII Exposure:**
       - Unencrypted emails, phone numbers, addresses
       - Social Security Numbers (SSNs)
       - Passport numbers, driver's licenses
       - Date of birth, personal identifiers
    
    2. **Sensitive Data Storage Violations:**
       - Plaintext passwords (check if properly hashed with bcrypt/scrypt)
       - API keys, access tokens, refresh tokens stored in database
       - JWT tokens, OAuth secrets
       - Private keys, certificates
    
    3. **PCI-DSS Violations (CRITICAL):**
       - Credit card numbers in any collection
       - CVV codes (should NEVER be stored)
       - Card expiration dates
       - Bank account numbers, routing numbers
    
    4. **HIPAA Violations:**
       - Medical records, diagnoses, prescriptions
       - Health insurance information
       - Patient identifiers without proper encryption
    
    5. **GDPR Compliance:**
       - Missing consent tracking fields (consent_date, consent_version)
       - No data processing legal basis documented
       - Lack of data retention policies
       - User data older than 2 years without justification
    
    6. **Data That Should NOT Be Stored:**
       - Full credit card numbers (use tokenization)
       - Plaintext passwords (must be hashed)
       - Unencrypted SSNs
       - Raw API keys (use secret management)
    
    For EACH violation found, report:
    - Collection name
    - Field name
    - Severity (Critical/High/Medium/Low)
    - Compliance framework violated (GDPR/PCI-DSS/HIPAA)
    - Specific recommendation (encryption, deletion, tokenization, etc.)
    - Sample document index (for investigation)
    
    Create a DETAILED Markdown security audit report with:
    1. **Executive Summary** - overall security posture (Critical/High/Medium/Low risk)
    2. **Infrastructure Security Findings** - Atlas configuration vulnerabilities
    3. **DATA COMPLIANCE VIOLATIONS** (NEW SECTION):
       - Total violations found (Critical/High/Medium/Low counts)
       - PII exposure details by collection
       - Sensitive data found (passwords, keys, tokens)
       - PCI-DSS violations (credit cards)
       - GDPR violations (missing consent, old data)
       - Specific field names and collections affected
    4. **Compliance Framework Summary:**
       - GDPR violations count and details
       - PCI-DSS violations count and details
       - HIPAA violations count and details
    5. **Risk Assessment** - prioritized list of ALL security risks with impact analysis
    6. **Remediation Roadmap** - 10-15 specific, actionable security improvements:
       - Infrastructure fixes (IP whitelist, encryption at rest, etc.)
       - Data compliance fixes (delete credit cards, hash passwords, encrypt PII)
       - Implementation priority (Immediate/High/Medium/Low)
       - Estimated effort (Easy/Medium/Hard)
    
    Be EXTREMELY CRITICAL. Every security gap and compliance violation must be identified.
    Include actual field names, collection names, and specific violation counts.
  expected_output: >
    Comprehensive Markdown security audit report (minimum 600 words) with:
    - Executive summary with overall risk rating
    - Infrastructure security findings (Atlas configuration)
    - DATA COMPLIANCE VIOLATIONS section with:
      * Violation counts by severity (Critical/High/Medium/Low)
      * Per-collection breakdown of violations
      * Specific field names containing sensitive data
      * PII exposure list
      * PCI-DSS/GDPR/HIPAA violations
    - Compliance framework summary (GDPR/PCI-DSS/HIPAA violation counts)
    - Risk assessment with business impact
    - 10-15 specific remediation steps prioritized by urgency
    Must include actual field names, collection names, and violation counts.
  agent: security_agent

cost_task:
  description: >
    Use atlas_cost_tool to perform a RUTHLESS, data-driven cost optimization analysis.
    
    For EACH cluster, analyze:
    1. Compute Costs - instance type (M0/M2/M5/M10/M20/M30/etc.), replication factor, shard count
    2. Storage Costs - disk size (GB), IOPS provisioned, volume type (STANDARD/PROVISIONED)
    3. Backup Costs - continuous backup enabled, PIT recovery, snapshot frequency, retention period
    4. Data Transfer - network ingress/egress patterns, cross-region transfer costs
    5. Autoscaling Configuration - min/max instance sizes, scaling triggers, efficiency
    6. Actual Utilization - 24-hour metrics: CPU %, memory %, connections, ops/sec, disk usage %
    
    Calculate cost waste by comparing:
    - Provisioned capacity vs actual usage (e.g., M10 but only 5% CPU = waste)
    - Backup costs vs actual recovery needs
    - Network costs from inefficient data access patterns
    - Idle or underutilized clusters costing money
    
    Create a DETAILED Markdown cost analysis report with:
    1. Executive Summary - total monthly spend estimate, total waste identified, potential savings
    2. Per-Cluster Cost Breakdown - for EACH cluster include:
       - Current configuration with estimated monthly cost in USD
       - Actual resource utilization with specific metrics (CPU %, memory %, disk %, ops/sec)
       - Overprovisioning analysis - exactly how much capacity is wasted
       - Right-sizing recommendation with new config and monthly savings in USD
    3. Backup Cost Analysis - current backup costs, optimization opportunities, savings
    4. Infrastructure Waste Report - idle clusters, oversized instances, unnecessary replicas
    5. Cost Optimization Roadmap - 7-10 specific actions with:
       - Current cost
       - Recommended change
       - Estimated monthly savings
       - Annual savings projection
       - Implementation effort (Easy/Medium/Hard)
    6. Summary Table - total potential monthly and annual savings
    
    Be EXTREMELY CRITICAL. Use actual MongoDB Atlas pricing. Calculate real dollar amounts.
    Every dollar of waste must be identified and quantified.
  expected_output: >
    Comprehensive Markdown cost analysis report (minimum 500 words) with executive summary showing
    total potential savings, detailed per-cluster cost breakdown with current spend vs optimized spend,
    backup cost analysis, infrastructure waste report, and 7-10 specific cost-cutting recommendations
    with estimated monthly and annual USD savings for each. Must include ACTUAL pricing calculations
    and utilization percentages. Include a final summary table of all savings opportunities.
  agent: cost_agent

schema_task:
  description: >
    Use mongodb_schema_tool to connect to the MongoDB database and perform COMPREHENSIVE schema analysis.

    The tool will automatically:
    1. Discover ALL collections in the database
    2. Sample documents (default 100 per collection) to infer schema
    3. Detect field types, nullable fields, and data type consistency
    4. Identify indexes on each collection
    5. Detect references between collections (foreign keys, one-to-many relationships)
    6. Analyze document sizes and collection statistics

    Your job as MongoDB Schema Architect is to:

    **SCHEMA ANALYSIS:**
    For EACH collection, analyze:
    - Total documents and storage size
    - Average document size (flag if >5KB as potential optimization target)
    - Field structure: data types, nested objects, arrays
    - Schema consistency: mixed types, null percentages
    - Embedded documents vs referenced documents

    **RELATIONSHIP MAPPING:**
    - Map out ALL relationships detected between collections
    - Identify foreign key patterns (fields ending in _id, _ref)
    - Detect one-to-many relationships (array of IDs)
    - Create a visual ASCII diagram of collection relationships
    - Flag potential circular references or overly complex relationships

    **INDEX ANALYSIS:**
    - List ALL existing indexes per collection
    - Identify missing indexes on foreign key fields
    - Flag collections without indexes on commonly queried fields
    - Detect redundant or unused indexes
    - Recommend compound indexes for common query patterns

    **DATA MODELING RECOMMENDATIONS:**
    Provide 10-15 specific recommendations covering:

    1. **Embedding vs Referencing:**
       - When to embed related data (1-to-1, 1-to-few with <100 items)
       - When to use references (1-to-many, many-to-many)
       - Identify collections that should be embedded for better performance

    2. **Denormalization Opportunities:**
       - Frequently joined data that should be denormalized
       - Read-heavy fields that could be duplicated to avoid lookups
       - Tradeoffs: write complexity vs read performance

    3. **Schema Optimization:**
       - Collections with too many fields (>50) that need refactoring
       - Large document sizes that should be split or use GridFS
       - Inconsistent field types that cause query issues
       - Arrays that have grown too large (>1000 items)

    4. **Index Strategy:**
       - Missing indexes causing slow queries
       - Compound indexes for multi-field queries
       - Text indexes for search functionality
       - Geospatial indexes if location data exists

    5. **Polymorphic Patterns:**
       - If collections have mixed document types, recommend discriminator patterns
       - Suggest schema versioning strategies

    6. **Query Pattern Optimization:**
       - Design schemas to match application query patterns
       - Identify anti-patterns (n+1 queries, unbounded arrays)
       - Suggest aggregation pipeline optimizations

    Create a DETAILED Markdown report with:
    1. **Executive Summary** - overall schema health, key findings
    2. **Collection Inventory** - table of all collections with stats
    3. **Schema Details** - per-collection deep dive with field types and indexes
    4. **Relationship Diagram** - ASCII or markdown visualization of collection relationships
    5. **Data Modeling Analysis** - evaluation of current patterns (embedding, referencing, denormalization)
    6. **Index Optimization** - comprehensive index recommendations with CREATE INDEX commands
    7. **Schema Refactoring Recommendations** - 10-15 specific improvements prioritized by impact
    8. **Migration Plan** - step-by-step guide for implementing recommended changes
    9. **Code Examples** - actual MongoDB commands and aggregation pipelines for migrations

    Be EXTREMELY detailed. Include actual field names, collection names, and executable commands.
    Compare current schema against MongoDB best practices and provide specific, actionable guidance.
  expected_output: >
    Comprehensive Markdown schema analysis report (minimum 500 words) with:
    - Executive summary of schema health
    - Complete collection inventory with statistics
    - Per-collection schema details with field types and indexes
    - Relationship diagram showing how collections reference each other
    - Data modeling evaluation (embedding vs referencing patterns)
    - 10-15 specific schema optimization recommendations with priority levels
    - Index recommendations with exact CREATE INDEX commands
    - Migration plan with step-by-step instructions
    - Code examples for schema refactoring
    Must include actual collection names, field names, and executable MongoDB commands.
  agent: schema_agent

synthesis_task:
  description: >
    Use report_synthesis_tool to read the comprehensive report.md file containing outputs from
    Performance, Security, and Cost agents. Perform intelligent synthesis and correlation:
    
    1. ANALYZE the complete report and extract:
       - All performance issues, bottlenecks, and metrics
       - All security vulnerabilities and risks
       - All cost inefficiencies and savings opportunities
    
    2. CALCULATE health scores (0-100) for each domain:
       - Performance Health Score (based on CPU, latency, bottlenecks, metrics availability)
       - Security Health Score (based on IP exposure, encryption, privileges, compliance)
       - Cost Efficiency Score (based on utilization, waste, overprovisioning)
       - Overall Health Score (weighted: 35% performance, 40% security, 25% cost)
    
    3. IDENTIFY cross-cutting concerns that span multiple domains:
       - Issues affecting both performance and cost (e.g., overprovisioned instances)
       - Security measures impacting performance (e.g., encryption overhead)
       - Cost optimizations with security implications (e.g., reducing backups)
    
    4. PRIORITIZE all findings by: Severity (Critical/High/Medium/Low) Ã— Business Impact
    
    5. DEDUPLICATE overlapping recommendations from different agents
    
    Create a comprehensive EXECUTIVE SUMMARY report with:
    1. Health Score Dashboard - overall score, per-domain scores, health rating (Excellent/Good/Fair/Poor/Critical)
    2. Executive Overview - 3-paragraph summary for C-level executives
    3. Critical Issues Requiring Immediate Action - top 5 issues with severity, impact, and urgency
    4. Cross-Domain Insights - correlations and dependencies between performance, security, and cost
    5. Prioritized Action Plan - consolidated roadmap with 10-15 actions ranked by ROI and urgency
    6. Risk Assessment - what happens if issues are not addressed (quantified business impact)
    7. Quick Wins - 3-5 easy optimizations that can be implemented immediately
    8. Long-term Strategic Recommendations - architectural improvements for next 6-12 months
    9. Comparison vs Best Practices - how this infrastructure compares to industry standards
    10. Metrics to Track - KPIs to monitor improvement over time
  expected_output: >
    Comprehensive executive summary report (minimum 600 words) with:
    - Health Score Dashboard (overall + per-domain scores with ratings)
    - 3-paragraph Executive Overview
    - Top 5 Critical Issues with severity ratings and business impact
    - Cross-domain correlation insights
    - Prioritized action plan with 10-15 consolidated recommendations ranked by ROI
    - Risk assessment with quantified business impact
    - 3-5 Quick Wins for immediate implementation
    - Long-term strategic recommendations
    - Best practices comparison
    - Success metrics and KPIs to track
    Must be data-driven, actionable, and executive-friendly.
  agent: report_synthesizer
